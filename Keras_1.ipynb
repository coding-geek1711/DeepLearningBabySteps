{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsgkVSY5fXPSYm2B4CHsIL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coding-geek1711/DeepLearningBabySteps/blob/master/Keras_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjFmf3ZyDEW-",
        "colab_type": "text"
      },
      "source": [
        "### ** So This is it...Keras **\n",
        "\n",
        "### Here we make a simple neural network and go until we make predictions and plot a confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IMLzv112rEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-NaJuIP3KF_",
        "colab_type": "text"
      },
      "source": [
        "###  Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re8CY7LH3UCP",
        "colab_type": "code",
        "outputId": "844e0e06-d2a9-4a4b-a174-50e8c17e6bef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ha9cy0s3-Zv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from random import randint\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLL9diLE331v",
        "colab_type": "text"
      },
      "source": [
        "We declare our labels now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbm2dOJV4R2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = []\n",
        "train_samples = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV97HV4s6CSI",
        "colab_type": "text"
      },
      "source": [
        "So now for the sake of the tutorial we create a dataset on our own, \n",
        "\n",
        "\n",
        "1.   A drug was tested on individuals from 13 to 100\n",
        "2.   Trial had 2100 people half were under 65 and half were above 65\n",
        "3.   95% of patients above 65 had side effects\n",
        "4.   95% of patients below 65 had no side effects\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-4PzUsG4sXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(50):\n",
        "  random_younger = randint(13, 64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_labels.append(1)\n",
        "\n",
        "  random_older = randint(65, 100)\n",
        "  train_samples.append(random_older)\n",
        "  train_labels.append(0)\n",
        "  \n",
        "for i in range(1000):\n",
        "  random_younger = randint(13, 64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_labels.append(0)\n",
        "\n",
        "  random_older = randint(65, 100)\n",
        "  train_samples.append(random_older)\n",
        "  train_labels.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvoVGtE15OCZ",
        "colab_type": "code",
        "outputId": "fd3601fe-b0a1-4052-d3bb-bdfe2ca9e841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_samples"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[18,\n",
              " 86,\n",
              " 61,\n",
              " 81,\n",
              " 47,\n",
              " 68,\n",
              " 32,\n",
              " 69,\n",
              " 35,\n",
              " 86,\n",
              " 41,\n",
              " 75,\n",
              " 36,\n",
              " 82,\n",
              " 50,\n",
              " 65,\n",
              " 36,\n",
              " 88,\n",
              " 46,\n",
              " 85,\n",
              " 13,\n",
              " 84,\n",
              " 53,\n",
              " 99,\n",
              " 25,\n",
              " 87,\n",
              " 53,\n",
              " 98,\n",
              " 34,\n",
              " 70,\n",
              " 33,\n",
              " 84,\n",
              " 53,\n",
              " 92,\n",
              " 60,\n",
              " 69,\n",
              " 36,\n",
              " 66,\n",
              " 23,\n",
              " 97,\n",
              " 56,\n",
              " 76,\n",
              " 42,\n",
              " 78,\n",
              " 47,\n",
              " 97,\n",
              " 33,\n",
              " 80,\n",
              " 27,\n",
              " 94,\n",
              " 52,\n",
              " 75,\n",
              " 21,\n",
              " 70,\n",
              " 29,\n",
              " 90,\n",
              " 14,\n",
              " 70,\n",
              " 44,\n",
              " 89,\n",
              " 32,\n",
              " 69,\n",
              " 64,\n",
              " 72,\n",
              " 45,\n",
              " 100,\n",
              " 24,\n",
              " 90,\n",
              " 26,\n",
              " 98,\n",
              " 24,\n",
              " 91,\n",
              " 51,\n",
              " 86,\n",
              " 53,\n",
              " 71,\n",
              " 41,\n",
              " 82,\n",
              " 26,\n",
              " 71,\n",
              " 52,\n",
              " 71,\n",
              " 29,\n",
              " 97,\n",
              " 33,\n",
              " 76,\n",
              " 23,\n",
              " 82,\n",
              " 49,\n",
              " 93,\n",
              " 42,\n",
              " 97,\n",
              " 25,\n",
              " 84,\n",
              " 25,\n",
              " 99,\n",
              " 52,\n",
              " 74,\n",
              " 34,\n",
              " 85,\n",
              " 35,\n",
              " 67,\n",
              " 60,\n",
              " 75,\n",
              " 53,\n",
              " 77,\n",
              " 60,\n",
              " 89,\n",
              " 26,\n",
              " 73,\n",
              " 50,\n",
              " 78,\n",
              " 32,\n",
              " 68,\n",
              " 29,\n",
              " 68,\n",
              " 59,\n",
              " 87,\n",
              " 24,\n",
              " 90,\n",
              " 49,\n",
              " 89,\n",
              " 40,\n",
              " 67,\n",
              " 63,\n",
              " 74,\n",
              " 35,\n",
              " 88,\n",
              " 62,\n",
              " 67,\n",
              " 26,\n",
              " 76,\n",
              " 48,\n",
              " 71,\n",
              " 46,\n",
              " 66,\n",
              " 18,\n",
              " 96,\n",
              " 23,\n",
              " 94,\n",
              " 19,\n",
              " 88,\n",
              " 41,\n",
              " 85,\n",
              " 53,\n",
              " 99,\n",
              " 43,\n",
              " 65,\n",
              " 32,\n",
              " 98,\n",
              " 62,\n",
              " 71,\n",
              " 40,\n",
              " 96,\n",
              " 64,\n",
              " 70,\n",
              " 23,\n",
              " 88,\n",
              " 60,\n",
              " 79,\n",
              " 49,\n",
              " 73,\n",
              " 41,\n",
              " 90,\n",
              " 50,\n",
              " 65,\n",
              " 17,\n",
              " 69,\n",
              " 44,\n",
              " 72,\n",
              " 40,\n",
              " 85,\n",
              " 55,\n",
              " 78,\n",
              " 56,\n",
              " 66,\n",
              " 45,\n",
              " 97,\n",
              " 39,\n",
              " 96,\n",
              " 21,\n",
              " 92,\n",
              " 20,\n",
              " 98,\n",
              " 28,\n",
              " 76,\n",
              " 23,\n",
              " 77,\n",
              " 35,\n",
              " 71,\n",
              " 35,\n",
              " 90,\n",
              " 20,\n",
              " 91,\n",
              " 39,\n",
              " 88,\n",
              " 56,\n",
              " 91,\n",
              " 25,\n",
              " 95,\n",
              " 45,\n",
              " 70,\n",
              " 47,\n",
              " 95,\n",
              " 41,\n",
              " 66,\n",
              " 34,\n",
              " 79,\n",
              " 40,\n",
              " 98,\n",
              " 51,\n",
              " 72,\n",
              " 30,\n",
              " 81,\n",
              " 41,\n",
              " 65,\n",
              " 51,\n",
              " 74,\n",
              " 20,\n",
              " 83,\n",
              " 23,\n",
              " 93,\n",
              " 32,\n",
              " 88,\n",
              " 59,\n",
              " 81,\n",
              " 37,\n",
              " 92,\n",
              " 45,\n",
              " 84,\n",
              " 38,\n",
              " 74,\n",
              " 41,\n",
              " 72,\n",
              " 26,\n",
              " 67,\n",
              " 20,\n",
              " 86,\n",
              " 48,\n",
              " 74,\n",
              " 22,\n",
              " 90,\n",
              " 34,\n",
              " 70,\n",
              " 28,\n",
              " 98,\n",
              " 61,\n",
              " 97,\n",
              " 57,\n",
              " 71,\n",
              " 47,\n",
              " 65,\n",
              " 52,\n",
              " 76,\n",
              " 34,\n",
              " 93,\n",
              " 39,\n",
              " 73,\n",
              " 48,\n",
              " 71,\n",
              " 33,\n",
              " 79,\n",
              " 24,\n",
              " 71,\n",
              " 22,\n",
              " 77,\n",
              " 19,\n",
              " 74,\n",
              " 57,\n",
              " 100,\n",
              " 55,\n",
              " 70,\n",
              " 58,\n",
              " 66,\n",
              " 50,\n",
              " 87,\n",
              " 60,\n",
              " 71,\n",
              " 21,\n",
              " 69,\n",
              " 39,\n",
              " 94,\n",
              " 45,\n",
              " 98,\n",
              " 35,\n",
              " 94,\n",
              " 41,\n",
              " 90,\n",
              " 20,\n",
              " 90,\n",
              " 27,\n",
              " 96,\n",
              " 37,\n",
              " 66,\n",
              " 41,\n",
              " 91,\n",
              " 50,\n",
              " 89,\n",
              " 26,\n",
              " 71,\n",
              " 20,\n",
              " 100,\n",
              " 24,\n",
              " 78,\n",
              " 44,\n",
              " 69,\n",
              " 27,\n",
              " 95,\n",
              " 25,\n",
              " 77,\n",
              " 52,\n",
              " 70,\n",
              " 28,\n",
              " 87,\n",
              " 31,\n",
              " 98,\n",
              " 40,\n",
              " 79,\n",
              " 31,\n",
              " 97,\n",
              " 47,\n",
              " 85,\n",
              " 44,\n",
              " 94,\n",
              " 63,\n",
              " 95,\n",
              " 63,\n",
              " 98,\n",
              " 28,\n",
              " 68,\n",
              " 30,\n",
              " 71,\n",
              " 14,\n",
              " 87,\n",
              " 32,\n",
              " 84,\n",
              " 15,\n",
              " 95,\n",
              " 48,\n",
              " 93,\n",
              " 15,\n",
              " 89,\n",
              " 54,\n",
              " 81,\n",
              " 46,\n",
              " 65,\n",
              " 36,\n",
              " 86,\n",
              " 37,\n",
              " 90,\n",
              " 28,\n",
              " 66,\n",
              " 22,\n",
              " 95,\n",
              " 31,\n",
              " 88,\n",
              " 33,\n",
              " 85,\n",
              " 48,\n",
              " 75,\n",
              " 24,\n",
              " 73,\n",
              " 31,\n",
              " 87,\n",
              " 14,\n",
              " 78,\n",
              " 17,\n",
              " 70,\n",
              " 32,\n",
              " 87,\n",
              " 39,\n",
              " 79,\n",
              " 62,\n",
              " 93,\n",
              " 47,\n",
              " 99,\n",
              " 40,\n",
              " 90,\n",
              " 28,\n",
              " 75,\n",
              " 17,\n",
              " 89,\n",
              " 31,\n",
              " 75,\n",
              " 42,\n",
              " 80,\n",
              " 44,\n",
              " 80,\n",
              " 48,\n",
              " 90,\n",
              " 20,\n",
              " 87,\n",
              " 21,\n",
              " 83,\n",
              " 54,\n",
              " 82,\n",
              " 31,\n",
              " 97,\n",
              " 22,\n",
              " 75,\n",
              " 35,\n",
              " 70,\n",
              " 36,\n",
              " 95,\n",
              " 27,\n",
              " 68,\n",
              " 57,\n",
              " 80,\n",
              " 46,\n",
              " 70,\n",
              " 38,\n",
              " 74,\n",
              " 31,\n",
              " 96,\n",
              " 43,\n",
              " 71,\n",
              " 14,\n",
              " 89,\n",
              " 56,\n",
              " 90,\n",
              " 20,\n",
              " 100,\n",
              " 41,\n",
              " 83,\n",
              " 53,\n",
              " 66,\n",
              " 40,\n",
              " 86,\n",
              " 20,\n",
              " 83,\n",
              " 36,\n",
              " 97,\n",
              " 34,\n",
              " 98,\n",
              " 39,\n",
              " 92,\n",
              " 23,\n",
              " 84,\n",
              " 46,\n",
              " 70,\n",
              " 43,\n",
              " 97,\n",
              " 62,\n",
              " 69,\n",
              " 37,\n",
              " 79,\n",
              " 57,\n",
              " 88,\n",
              " 26,\n",
              " 74,\n",
              " 37,\n",
              " 69,\n",
              " 51,\n",
              " 97,\n",
              " 20,\n",
              " 66,\n",
              " 62,\n",
              " 84,\n",
              " 35,\n",
              " 85,\n",
              " 17,\n",
              " 81,\n",
              " 62,\n",
              " 100,\n",
              " 46,\n",
              " 99,\n",
              " 49,\n",
              " 87,\n",
              " 48,\n",
              " 73,\n",
              " 61,\n",
              " 68,\n",
              " 48,\n",
              " 89,\n",
              " 40,\n",
              " 87,\n",
              " 41,\n",
              " 88,\n",
              " 17,\n",
              " 92,\n",
              " 16,\n",
              " 80,\n",
              " 37,\n",
              " 85,\n",
              " 41,\n",
              " 92,\n",
              " 54,\n",
              " 65,\n",
              " 31,\n",
              " 85,\n",
              " 27,\n",
              " 80,\n",
              " 59,\n",
              " 78,\n",
              " 34,\n",
              " 94,\n",
              " 41,\n",
              " 77,\n",
              " 57,\n",
              " 72,\n",
              " 62,\n",
              " 66,\n",
              " 21,\n",
              " 72,\n",
              " 18,\n",
              " 95,\n",
              " 43,\n",
              " 65,\n",
              " 31,\n",
              " 80,\n",
              " 33,\n",
              " 82,\n",
              " 17,\n",
              " 71,\n",
              " 55,\n",
              " 94,\n",
              " 30,\n",
              " 99,\n",
              " 20,\n",
              " 95,\n",
              " 30,\n",
              " 98,\n",
              " 36,\n",
              " 75,\n",
              " 64,\n",
              " 75,\n",
              " 18,\n",
              " 87,\n",
              " 40,\n",
              " 81,\n",
              " 39,\n",
              " 96,\n",
              " 48,\n",
              " 69,\n",
              " 13,\n",
              " 87,\n",
              " 43,\n",
              " 89,\n",
              " 32,\n",
              " 92,\n",
              " 60,\n",
              " 95,\n",
              " 41,\n",
              " 95,\n",
              " 47,\n",
              " 84,\n",
              " 19,\n",
              " 81,\n",
              " 48,\n",
              " 65,\n",
              " 18,\n",
              " 83,\n",
              " 63,\n",
              " 65,\n",
              " 57,\n",
              " 79,\n",
              " 36,\n",
              " 76,\n",
              " 55,\n",
              " 73,\n",
              " 43,\n",
              " 66,\n",
              " 47,\n",
              " 76,\n",
              " 46,\n",
              " 96,\n",
              " 49,\n",
              " 92,\n",
              " 42,\n",
              " 67,\n",
              " 17,\n",
              " 75,\n",
              " 41,\n",
              " 88,\n",
              " 37,\n",
              " 76,\n",
              " 39,\n",
              " 94,\n",
              " 28,\n",
              " 70,\n",
              " 51,\n",
              " 97,\n",
              " 47,\n",
              " 85,\n",
              " 43,\n",
              " 74,\n",
              " 19,\n",
              " 76,\n",
              " 60,\n",
              " 93,\n",
              " 51,\n",
              " 73,\n",
              " 43,\n",
              " 94,\n",
              " 28,\n",
              " 87,\n",
              " 43,\n",
              " 85,\n",
              " 20,\n",
              " 66,\n",
              " 32,\n",
              " 83,\n",
              " 47,\n",
              " 85,\n",
              " 60,\n",
              " 71,\n",
              " 37,\n",
              " 79,\n",
              " 45,\n",
              " 90,\n",
              " 50,\n",
              " 71,\n",
              " 50,\n",
              " 82,\n",
              " 58,\n",
              " 91,\n",
              " 13,\n",
              " 77,\n",
              " 42,\n",
              " 98,\n",
              " 40,\n",
              " 81,\n",
              " 32,\n",
              " 96,\n",
              " 28,\n",
              " 75,\n",
              " 55,\n",
              " 71,\n",
              " 50,\n",
              " 79,\n",
              " 50,\n",
              " 96,\n",
              " 20,\n",
              " 94,\n",
              " 43,\n",
              " 91,\n",
              " 24,\n",
              " 90,\n",
              " 52,\n",
              " 90,\n",
              " 35,\n",
              " 73,\n",
              " 19,\n",
              " 83,\n",
              " 57,\n",
              " 94,\n",
              " 29,\n",
              " 78,\n",
              " 48,\n",
              " 69,\n",
              " 19,\n",
              " 97,\n",
              " 45,\n",
              " 66,\n",
              " 20,\n",
              " 97,\n",
              " 18,\n",
              " 79,\n",
              " 13,\n",
              " 76,\n",
              " 32,\n",
              " 90,\n",
              " 59,\n",
              " 79,\n",
              " 47,\n",
              " 70,\n",
              " 41,\n",
              " 76,\n",
              " 51,\n",
              " 66,\n",
              " 26,\n",
              " 66,\n",
              " 39,\n",
              " 78,\n",
              " 48,\n",
              " 92,\n",
              " 17,\n",
              " 68,\n",
              " 51,\n",
              " 85,\n",
              " 29,\n",
              " 88,\n",
              " 39,\n",
              " 94,\n",
              " 55,\n",
              " 99,\n",
              " 20,\n",
              " 99,\n",
              " 30,\n",
              " 74,\n",
              " 47,\n",
              " 81,\n",
              " 33,\n",
              " 95,\n",
              " 22,\n",
              " 90,\n",
              " 23,\n",
              " 95,\n",
              " 63,\n",
              " 85,\n",
              " 36,\n",
              " 65,\n",
              " 19,\n",
              " 94,\n",
              " 41,\n",
              " 90,\n",
              " 38,\n",
              " 90,\n",
              " 53,\n",
              " 81,\n",
              " 47,\n",
              " 76,\n",
              " 19,\n",
              " 89,\n",
              " 57,\n",
              " 90,\n",
              " 45,\n",
              " 70,\n",
              " 64,\n",
              " 69,\n",
              " 47,\n",
              " 74,\n",
              " 55,\n",
              " 97,\n",
              " 30,\n",
              " 79,\n",
              " 54,\n",
              " 85,\n",
              " 55,\n",
              " 65,\n",
              " 64,\n",
              " 96,\n",
              " 50,\n",
              " 96,\n",
              " 39,\n",
              " 85,\n",
              " 44,\n",
              " 74,\n",
              " 54,\n",
              " 95,\n",
              " 24,\n",
              " 92,\n",
              " 22,\n",
              " 82,\n",
              " 21,\n",
              " 82,\n",
              " 54,\n",
              " 72,\n",
              " 56,\n",
              " 99,\n",
              " 40,\n",
              " 89,\n",
              " 41,\n",
              " 91,\n",
              " 17,\n",
              " 81,\n",
              " 44,\n",
              " 65,\n",
              " 49,\n",
              " 70,\n",
              " 22,\n",
              " 66,\n",
              " 42,\n",
              " 92,\n",
              " 14,\n",
              " 79,\n",
              " 44,\n",
              " 76,\n",
              " 36,\n",
              " 74,\n",
              " 52,\n",
              " 97,\n",
              " 61,\n",
              " 73,\n",
              " 21,\n",
              " 90,\n",
              " 31,\n",
              " 86,\n",
              " 31,\n",
              " 75,\n",
              " 60,\n",
              " 78,\n",
              " 30,\n",
              " 83,\n",
              " 28,\n",
              " 93,\n",
              " 28,\n",
              " 77,\n",
              " 62,\n",
              " 91,\n",
              " 14,\n",
              " 100,\n",
              " 24,\n",
              " 79,\n",
              " 25,\n",
              " 83,\n",
              " 35,\n",
              " 79,\n",
              " 41,\n",
              " 83,\n",
              " 44,\n",
              " 71,\n",
              " 61,\n",
              " 79,\n",
              " 51,\n",
              " 99,\n",
              " 23,\n",
              " 77,\n",
              " 37,\n",
              " 78,\n",
              " 26,\n",
              " 90,\n",
              " 61,\n",
              " 65,\n",
              " 18,\n",
              " 88,\n",
              " 37,\n",
              " 89,\n",
              " 16,\n",
              " 82,\n",
              " 16,\n",
              " 65,\n",
              " 16,\n",
              " 69,\n",
              " 30,\n",
              " 98,\n",
              " 24,\n",
              " 79,\n",
              " 60,\n",
              " 88,\n",
              " 27,\n",
              " 80,\n",
              " 25,\n",
              " 66,\n",
              " 56,\n",
              " 76,\n",
              " 56,\n",
              " 100,\n",
              " 60,\n",
              " 99,\n",
              " 15,\n",
              " 96,\n",
              " 30,\n",
              " 68,\n",
              " 28,\n",
              " 88,\n",
              " 20,\n",
              " 80,\n",
              " 64,\n",
              " 89,\n",
              " 22,\n",
              " 79,\n",
              " 54,\n",
              " 71,\n",
              " 62,\n",
              " 84,\n",
              " 45,\n",
              " 77,\n",
              " 35,\n",
              " 100,\n",
              " 34,\n",
              " 88,\n",
              " 34,\n",
              " 76,\n",
              " 46,\n",
              " 85,\n",
              " 41,\n",
              " 66,\n",
              " 58,\n",
              " 89,\n",
              " 27,\n",
              " 85,\n",
              " 50,\n",
              " 95,\n",
              " 54,\n",
              " 90,\n",
              " 32,\n",
              " 72,\n",
              " 62,\n",
              " 65,\n",
              " 17,\n",
              " 67,\n",
              " 61,\n",
              " 76,\n",
              " 41,\n",
              " 93,\n",
              " 13,\n",
              " 99,\n",
              " 34,\n",
              " 69,\n",
              " 20,\n",
              " 67,\n",
              " 53,\n",
              " 82,\n",
              " 59,\n",
              " 73,\n",
              " 38,\n",
              " 68,\n",
              " 42,\n",
              " 85,\n",
              " 27,\n",
              " 89,\n",
              " 49,\n",
              " 70,\n",
              " 54,\n",
              " 69,\n",
              " 58,\n",
              " 81,\n",
              " 50,\n",
              " 71,\n",
              " 25,\n",
              " 96,\n",
              " 13,\n",
              " 87,\n",
              " 25,\n",
              " 86,\n",
              " 43,\n",
              " 88,\n",
              " 43,\n",
              " 67,\n",
              " 29,\n",
              " 88,\n",
              " 61,\n",
              " 77,\n",
              " 49,\n",
              " 86,\n",
              " 34,\n",
              " 91,\n",
              " 58,\n",
              " 98,\n",
              " 39,\n",
              " 67,\n",
              " 13,\n",
              " 75,\n",
              " 58,\n",
              " 81,\n",
              " 38,\n",
              " 86,\n",
              " 61,\n",
              " 81,\n",
              " 27,\n",
              " 71,\n",
              " 15,\n",
              " 81,\n",
              " 32,\n",
              " 66,\n",
              " 17,\n",
              " 67,\n",
              " 64,\n",
              " 94,\n",
              " 15,\n",
              " 65,\n",
              " 53,\n",
              " 77,\n",
              " 53,\n",
              " 66,\n",
              " 49,\n",
              " 65,\n",
              " 35,\n",
              " 95,\n",
              " 21,\n",
              " 82,\n",
              " 26,\n",
              " 66,\n",
              " 20,\n",
              " 88,\n",
              " 29,\n",
              " 78,\n",
              " 15,\n",
              " 81,\n",
              " 36,\n",
              " 78,\n",
              " 44,\n",
              " 96,\n",
              " 24,\n",
              " 88,\n",
              " 17,\n",
              " 79,\n",
              " 47,\n",
              " 85,\n",
              " 59,\n",
              " 73,\n",
              " 19,\n",
              " 97,\n",
              " 62,\n",
              " 74,\n",
              " 26,\n",
              " 70,\n",
              " 33,\n",
              " 91,\n",
              " 50,\n",
              " 92,\n",
              " 35,\n",
              " 85,\n",
              " 33,\n",
              " 81,\n",
              " 41,\n",
              " 82,\n",
              " 21,\n",
              " 89,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KgckWWe5V5T",
        "colab_type": "code",
        "outputId": "2f5c7b8b-31b0-4b71-eb26-a09f0ca9cc5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQxotlXn7k5-",
        "colab_type": "text"
      },
      "source": [
        "We make a numpy array for keras to understand things\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q12rb9hF7P1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = np.array(train_labels)\n",
        "train_samples = np.array(train_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UQjQ7br7ufG",
        "colab_type": "text"
      },
      "source": [
        "Now we use scikit learns MinMaxScaler to convert numbers ranging from 13 to 100 to numbers between 0 and 1\n",
        "\n",
        "Also fit_transform function doesnt accept 1D array, so we have to convert using reshape.![alt text](https://drive.google.com/uc?id=1CU9QseZ6fupI471toMiEkkSs_rafp6a9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8aVDAeS78Z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_train_samples = scaler.fit_transform((train_samples.reshape(-1, 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFzwMulx9zPA",
        "colab_type": "code",
        "outputId": "5f131c2d-5575-4d66-be7f-d5661981b691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "scaled_train_samples"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.05747126],\n",
              "       [0.83908046],\n",
              "       [0.55172414],\n",
              "       ...,\n",
              "       [0.97701149],\n",
              "       [0.03448276],\n",
              "       [0.65517241]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_hr6hJv-Koe",
        "colab_type": "text"
      },
      "source": [
        "### Now We Create a simple sequential model with keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEeE16-h-QGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import categorical_crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vARUui6d-3Ne",
        "colab_type": "text"
      },
      "source": [
        "Keras Sequential Model is a model where each layer is part of an array which upon execution is executed sequentially\n",
        "\n",
        "WAYS TO ADD A LAYER \n",
        "\n",
        "*   model = Sequential([l1, l2, l3, l4]) as per ur desired sequence\n",
        "*   model.add(layer_number)\n",
        "*   model.app(layer_number)\n",
        "\n",
        "First layer is going to be the Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stcvab68_uZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "                    Dense(16, input_shape = (1, ), activation = 'relu'),       ### Dense(number of neurons, '''if its the first layer then only''' input_shape = (shape of data input), activation = 'activation function in single quotes' )\n",
        "                    Dense(32, activation='relu'), \n",
        "                    Dense(2, activation='softmax')                                                                        ### This is gonna be the output layer from earlier and hence they have only two units, either a yes to side effect or a no to side effect\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSpiOFOSB0aM",
        "colab_type": "text"
      },
      "source": [
        "Now our Keras Sequential model is created , now to see its summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2udxX8zBkwl",
        "colab_type": "code",
        "outputId": "657f99bd-657d-48f4-c563-8a50f89a4f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "model.summary()       # the summary of the model is shown"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 16)                32        \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 642\n",
            "Trainable params: 642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q67tZlZECKms",
        "colab_type": "text"
      },
      "source": [
        "Now we train our dataset on this model which we created\n",
        "\n",
        "\n",
        "First we need to compile our model\n",
        "\n",
        "### So, we pass a few arguments first is the optimizer, we can use several but now we use Adam, which if used has to be given a learning rate lr, then the next argument is loss, again there are several documented in keras, and last argument is metrics, which are also documeneted in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8TyOcIQCOa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9As_9ygVEZAY",
        "colab_type": "text"
      },
      "source": [
        "### model.fit(training_data, training_data_labels, batch_size=arbitrarily chosen batch size, epochs = number of goes through our dataset, shuffle = True means each epoch the dataset is randomized to avoid training on the same data over and over again, verbose = 2 means how much info to be displayed during training)\n",
        "\n",
        "batch size means the amount of dataset chosen at one epoch to be trained upon\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSVb7NLqDXwL",
        "colab_type": "code",
        "outputId": "8ce51404-8bdd-4d41-910e-b2e6a4d0f426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "model.fit(scaled_train_samples, train_labels,batch_size=10, epochs=20, shuffle = True, verbose=2)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            " - 0s - loss: 0.2642 - acc: 0.9756\n",
            "Epoch 2/20\n",
            " - 0s - loss: 0.2531 - acc: 0.9746\n",
            "Epoch 3/20\n",
            " - 0s - loss: 0.2440 - acc: 0.9751\n",
            "Epoch 4/20\n",
            " - 0s - loss: 0.2356 - acc: 0.9756\n",
            "Epoch 5/20\n",
            " - 0s - loss: 0.2274 - acc: 0.9756\n",
            "Epoch 6/20\n",
            " - 0s - loss: 0.2193 - acc: 0.9756\n",
            "Epoch 7/20\n",
            " - 0s - loss: 0.2112 - acc: 0.9756\n",
            "Epoch 8/20\n",
            " - 0s - loss: 0.2040 - acc: 0.9756\n",
            "Epoch 9/20\n",
            " - 0s - loss: 0.1966 - acc: 0.9751\n",
            "Epoch 10/20\n",
            " - 0s - loss: 0.1896 - acc: 0.9756\n",
            "Epoch 11/20\n",
            " - 0s - loss: 0.1840 - acc: 0.9756\n",
            "Epoch 12/20\n",
            " - 0s - loss: 0.1797 - acc: 0.9756\n",
            "Epoch 13/20\n",
            " - 0s - loss: 0.1762 - acc: 0.9756\n",
            "Epoch 14/20\n",
            " - 0s - loss: 0.1736 - acc: 0.9756\n",
            "Epoch 15/20\n",
            " - 0s - loss: 0.1715 - acc: 0.9756\n",
            "Epoch 16/20\n",
            " - 0s - loss: 0.1700 - acc: 0.9756\n",
            "Epoch 17/20\n",
            " - 0s - loss: 0.1684 - acc: 0.9756\n",
            "Epoch 18/20\n",
            " - 0s - loss: 0.1670 - acc: 0.9756\n",
            "Epoch 19/20\n",
            " - 0s - loss: 0.1656 - acc: 0.9756\n",
            "Epoch 20/20\n",
            " - 0s - loss: 0.1645 - acc: 0.9756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f698e181630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubSgUxy9FMzk",
        "colab_type": "text"
      },
      "source": [
        "### Now if we look at the results above we see that the loss steadily decreases and the accuracy steadily increases, epoch_1 acc ~ 59% and epoch _20 acc ~ 96% epoch_1 loss = 0.66 and epoch_20 loss = 0.17....therefore as epochs increase, loss decreases and accuracy increases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRNvhnncFyk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp1R6vjDGaA0",
        "colab_type": "text"
      },
      "source": [
        "### VALIDATION SET CREATION\n",
        "Validation set are those part of our dataset upon which the model is not trained ie we set them aside. The aim to do so arises from the basic requirement of the neural net to learn from our training data and predict something which it has not trained on.\n",
        "\n",
        "\n",
        "The way to do so is to split our dataset into two parts, the training part and the validation part.What happens then is the net tests itself after training on the training dataset on the validation dataset every single epoch and the loss and accuracy of both the training and validation dataset is observed in the model.fit(). This allow u to see how well ur model is able to generaize and also it helps to know whether our model is overfitting.\n",
        "\n",
        "Overfittiing generally means that our model is only learning the specificities for our training dataset and does not generalize well on the data outside the training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfgPPUo8ICXj",
        "colab_type": "text"
      },
      "source": [
        "The ways to use the valid_set is as follows:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   First we declare a validation set as follows\n",
        "valid_set = [(sample, label), (sample, label), .....(sample, label)]\n",
        "Then we put that in model.fit as follows\n",
        "model.fit(scaled_train_samples, train_labels, **validation_data=valid_set**, batch_size=10, epochs=20, shuffle = True, verbose=2)   \n",
        "\n",
        "\n",
        "2. Or we can do this \n",
        "model.fit(scaled_train_samples, train_labels,** validation_split=0.1, **batch_size=10, epochs=20, shuffle = True, verbose=2)      \n",
        "\n",
        "\n",
        "What validation_split does is it splits our training data into two parts, 90% on which it trains and 10% on which it validates, the 0.1 can be adjusted as per desire\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnTr1txhIyPj",
        "colab_type": "code",
        "outputId": "6f714d43-9af3-4e80-fbae-5b5d6c05febd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "source": [
        "model.fit(scaled_train_samples, train_labels, validation_split = 0.1, batch_size=10, epochs=20, shuffle = True, verbose=2)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1890 samples, validate on 210 samples\n",
            "Epoch 1/20\n",
            " - 0s - loss: 0.6737 - acc: 0.5360 - val_loss: 0.6528 - val_acc: 0.5762\n",
            "Epoch 2/20\n",
            " - 0s - loss: 0.6479 - acc: 0.5921 - val_loss: 0.6224 - val_acc: 0.6333\n",
            "Epoch 3/20\n",
            " - 0s - loss: 0.6247 - acc: 0.6365 - val_loss: 0.5945 - val_acc: 0.7000\n",
            "Epoch 4/20\n",
            " - 0s - loss: 0.6021 - acc: 0.6820 - val_loss: 0.5665 - val_acc: 0.7476\n",
            "Epoch 5/20\n",
            " - 0s - loss: 0.5794 - acc: 0.7201 - val_loss: 0.5378 - val_acc: 0.7810\n",
            "Epoch 6/20\n",
            " - 0s - loss: 0.5560 - acc: 0.7593 - val_loss: 0.5078 - val_acc: 0.8286\n",
            "Epoch 7/20\n",
            " - 0s - loss: 0.5290 - acc: 0.7910 - val_loss: 0.4730 - val_acc: 0.8571\n",
            "Epoch 8/20\n",
            " - 0s - loss: 0.5018 - acc: 0.8233 - val_loss: 0.4404 - val_acc: 0.9000\n",
            "Epoch 9/20\n",
            " - 0s - loss: 0.4758 - acc: 0.8471 - val_loss: 0.4075 - val_acc: 0.9143\n",
            "Epoch 10/20\n",
            " - 0s - loss: 0.4505 - acc: 0.8577 - val_loss: 0.3767 - val_acc: 0.9286\n",
            "Epoch 11/20\n",
            " - 0s - loss: 0.4271 - acc: 0.8735 - val_loss: 0.3480 - val_acc: 0.9333\n",
            "Epoch 12/20\n",
            " - 0s - loss: 0.4058 - acc: 0.8820 - val_loss: 0.3219 - val_acc: 0.9619\n",
            "Epoch 13/20\n",
            " - 0s - loss: 0.3868 - acc: 0.8926 - val_loss: 0.2972 - val_acc: 0.9619\n",
            "Epoch 14/20\n",
            " - 0s - loss: 0.3702 - acc: 0.9000 - val_loss: 0.2755 - val_acc: 0.9619\n",
            "Epoch 15/20\n",
            " - 0s - loss: 0.3559 - acc: 0.9069 - val_loss: 0.2564 - val_acc: 0.9714\n",
            "Epoch 16/20\n",
            " - 0s - loss: 0.3435 - acc: 0.9090 - val_loss: 0.2405 - val_acc: 0.9714\n",
            "Epoch 17/20\n",
            " - 0s - loss: 0.3331 - acc: 0.9101 - val_loss: 0.2258 - val_acc: 0.9714\n",
            "Epoch 18/20\n",
            " - 0s - loss: 0.3245 - acc: 0.9148 - val_loss: 0.2128 - val_acc: 0.9714\n",
            "Epoch 19/20\n",
            " - 0s - loss: 0.3169 - acc: 0.9196 - val_loss: 0.2020 - val_acc: 0.9714\n",
            "Epoch 20/20\n",
            " - 0s - loss: 0.3107 - acc: 0.9212 - val_loss: 0.1920 - val_acc: 0.9714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6986fe1ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7OPaunQKZXh",
        "colab_type": "text"
      },
      "source": [
        "This is not a case of Overfitting of data, \n",
        "\n",
        "In overfitting the models accuracy on training side is increasing but stagnates on the validation side, \n",
        "\n",
        "and \n",
        "\n",
        "the models loss on the training side decreases on the training side but is actually increasing on the validation side\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O92fkAAiZ1pZ",
        "colab_type": "text"
      },
      "source": [
        "### **PREDICT** \n",
        "We need to preprocess test data which we use to predict our side effect occurence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvq48bkKaxfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### preprocessing\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "for i in range(10):\n",
        "  random_younger = randint(13, 64)\n",
        "  test_samples.append(random_younger)\n",
        "  test_labels.append(1)\n",
        "\n",
        "  random_older = randint(65, 100)\n",
        "  test_samples.append(random_older)\n",
        "  test_labels.append(0)\n",
        "\n",
        "for i in range(200):\n",
        "  random_younger = randint(13, 64)\n",
        "  test_samples.append(random_younger)\n",
        "  test_labels.append(0)\n",
        "\n",
        "  random_older = randint(65, 100)\n",
        "  test_samples.append(random_older)\n",
        "  test_labels.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEKF0Ay_boCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_samples = np.array(test_samples)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pGbGTiebwDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghqv3Ud1cDZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### prediction\n",
        "prediction = model.predict(scaled_test_samples, batch_size=10, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOMR_-Q7crwf",
        "colab_type": "code",
        "outputId": "5589182f-9068-48e5-f013-ef311ac84a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "prediction"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.85902387, 0.14097606],\n",
              "       [0.08653699, 0.91346294],\n",
              "       [0.48869094, 0.5113091 ],\n",
              "       [0.09378345, 0.9062165 ],\n",
              "       [0.915371  , 0.084629  ],\n",
              "       [0.04737702, 0.95262295],\n",
              "       [0.91582227, 0.08417773],\n",
              "       [0.06547555, 0.9345244 ],\n",
              "       [0.7943509 , 0.20564902],\n",
              "       [0.05280788, 0.94719213],\n",
              "       [0.915371  , 0.084629  ],\n",
              "       [0.07677455, 0.9232254 ],\n",
              "       [0.8963086 , 0.10369135],\n",
              "       [0.27362463, 0.72637534],\n",
              "       [0.9155647 , 0.08443531],\n",
              "       [0.07677455, 0.9232254 ],\n",
              "       [0.8766623 , 0.1233377 ],\n",
              "       [0.12927605, 0.8707239 ],\n",
              "       [0.900467  , 0.09953302],\n",
              "       [0.06905862, 0.9309414 ],\n",
              "       [0.8825958 , 0.11740423],\n",
              "       [0.25551414, 0.74448586],\n",
              "       [0.8485916 , 0.1514084 ],\n",
              "       [0.11915231, 0.88084763],\n",
              "       [0.44239268, 0.5576073 ],\n",
              "       [0.35345578, 0.6465442 ],\n",
              "       [0.8231108 , 0.17688914],\n",
              "       [0.06547555, 0.9345244 ],\n",
              "       [0.7449836 , 0.25501636],\n",
              "       [0.04737702, 0.95262295],\n",
              "       [0.8362619 , 0.16373803],\n",
              "       [0.07677455, 0.9232254 ],\n",
              "       [0.64714247, 0.35285756],\n",
              "       [0.04257717, 0.95742285],\n",
              "       [0.8485916 , 0.1514084 ],\n",
              "       [0.08114623, 0.9188537 ],\n",
              "       [0.89199746, 0.10800254],\n",
              "       [0.14012301, 0.859877  ],\n",
              "       [0.91582227, 0.08417774],\n",
              "       [0.04737702, 0.95262295],\n",
              "       [0.89199746, 0.10800254],\n",
              "       [0.05280788, 0.94719213],\n",
              "       [0.8766623 , 0.1233377 ],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.668101  , 0.33189902],\n",
              "       [0.05002266, 0.9499774 ],\n",
              "       [0.8766623 , 0.1233377 ],\n",
              "       [0.16409688, 0.8359031 ],\n",
              "       [0.7943509 , 0.20564902],\n",
              "       [0.04737702, 0.95262295],\n",
              "       [0.88752943, 0.11247052],\n",
              "       [0.04486468, 0.95513535],\n",
              "       [0.91550016, 0.08449984],\n",
              "       [0.12927605, 0.8707239 ],\n",
              "       [0.64714247, 0.35285756],\n",
              "       [0.37501147, 0.6249885 ],\n",
              "       [0.7787244 , 0.22127563],\n",
              "       [0.23820917, 0.7617908 ],\n",
              "       [0.86866534, 0.13133465],\n",
              "       [0.19126026, 0.8087398 ],\n",
              "       [0.6256021 , 0.3743979 ],\n",
              "       [0.06547555, 0.9345244 ],\n",
              "       [0.91530484, 0.08469514],\n",
              "       [0.05882284, 0.9411771 ],\n",
              "       [0.7449836 , 0.25501636],\n",
              "       [0.19126026, 0.8087398 ],\n",
              "       [0.6256021 , 0.3743979 ],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.535184  , 0.46481597],\n",
              "       [0.16409686, 0.8359031 ],\n",
              "       [0.9130836 , 0.08691638],\n",
              "       [0.17727074, 0.8227293 ],\n",
              "       [0.86866534, 0.13133465],\n",
              "       [0.04737702, 0.95262295],\n",
              "       [0.8231108 , 0.17688914],\n",
              "       [0.15172157, 0.8482784 ],\n",
              "       [0.8766623 , 0.1233377 ],\n",
              "       [0.05882284, 0.9411771 ],\n",
              "       [0.5810736 , 0.4189264 ],\n",
              "       [0.05280788, 0.94719213],\n",
              "       [0.8963086 , 0.10369135],\n",
              "       [0.11915231, 0.88084763],\n",
              "       [0.9149825 , 0.08501752],\n",
              "       [0.33247992, 0.6675201 ],\n",
              "       [0.9157579 , 0.08424205],\n",
              "       [0.06905862, 0.9309414 ],\n",
              "       [0.85902387, 0.14097606],\n",
              "       [0.31214792, 0.6878521 ],\n",
              "       [0.6256021 , 0.3743979 ],\n",
              "       [0.04257717, 0.95742285],\n",
              "       [0.668101  , 0.33189902],\n",
              "       [0.15172157, 0.8482784 ],\n",
              "       [0.9151769 , 0.08482305],\n",
              "       [0.08653699, 0.91346294],\n",
              "       [0.91562915, 0.08437086],\n",
              "       [0.05280788, 0.94719213],\n",
              "       [0.8963086 , 0.10369135],\n",
              "       [0.04041795, 0.9595821 ],\n",
              "       [0.9151769 , 0.08482305],\n",
              "       [0.05280788, 0.94719213],\n",
              "       [0.9091369 , 0.09086304],\n",
              "       [0.06905862, 0.9309414 ],\n",
              "       [0.668101  , 0.33189902],\n",
              "       [0.12927605, 0.8707239 ],\n",
              "       [0.91427296, 0.08572707],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.91504735, 0.08495267],\n",
              "       [0.06547555, 0.9345244 ],\n",
              "       [0.91562915, 0.08437086],\n",
              "       [0.11915231, 0.88084763],\n",
              "       [0.91582227, 0.08417773],\n",
              "       [0.07677455, 0.9232254 ],\n",
              "       [0.9149825 , 0.08501752],\n",
              "       [0.25551414, 0.74448586],\n",
              "       [0.86866534, 0.13133465],\n",
              "       [0.05280788, 0.94719213],\n",
              "       [0.9153064 , 0.08469363],\n",
              "       [0.04486468, 0.95513535],\n",
              "       [0.9149825 , 0.08501752],\n",
              "       [0.07677455, 0.9232254 ],\n",
              "       [0.7449836 , 0.25501636],\n",
              "       [0.04041795, 0.9595821 ],\n",
              "       [0.91562915, 0.08437086],\n",
              "       [0.20607713, 0.79392284],\n",
              "       [0.8362619 , 0.16373803],\n",
              "       [0.11003025, 0.8899697 ],\n",
              "       [0.900467  , 0.09953302],\n",
              "       [0.27362463, 0.72637534],\n",
              "       [0.91524166, 0.08475833],\n",
              "       [0.062066  , 0.93793404],\n",
              "       [0.48869094, 0.5113091 ],\n",
              "       [0.27362463, 0.72637534],\n",
              "       [0.86866534, 0.13133465],\n",
              "       [0.14012301, 0.859877  ],\n",
              "       [0.5119634 , 0.48803654],\n",
              "       [0.22172725, 0.77827275],\n",
              "       [0.91427296, 0.08572707],\n",
              "       [0.10161927, 0.89838076],\n",
              "       [0.9149825 , 0.08501752],\n",
              "       [0.05882284, 0.9411771 ],\n",
              "       [0.9111302 , 0.08886982],\n",
              "       [0.39707443, 0.60292554],\n",
              "       [0.8485916 , 0.1514084 ],\n",
              "       [0.04257717, 0.95742285],\n",
              "       [0.9091369 , 0.09086304],\n",
              "       [0.20607713, 0.79392284],\n",
              "       [0.7080271 , 0.29197285],\n",
              "       [0.20607713, 0.79392284],\n",
              "       [0.7943509 , 0.20564902],\n",
              "       [0.19126026, 0.8087398 ],\n",
              "       [0.48869094, 0.5113091 ],\n",
              "       [0.04041795, 0.9595821 ],\n",
              "       [0.9153064 , 0.08469363],\n",
              "       [0.12927605, 0.8707239 ],\n",
              "       [0.915371  , 0.084629  ],\n",
              "       [0.25551414, 0.74448586],\n",
              "       [0.41956392, 0.5804361 ],\n",
              "       [0.09378345, 0.9062165 ],\n",
              "       [0.900467  , 0.09953302],\n",
              "       [0.04737702, 0.95262295],\n",
              "       [0.44239268, 0.5576073 ],\n",
              "       [0.22172725, 0.77827275],\n",
              "       [0.91562915, 0.08437086],\n",
              "       [0.10161927, 0.89838076],\n",
              "       [0.8485916 , 0.1514084 ],\n",
              "       [0.07677455, 0.9232254 ],\n",
              "       [0.6035524 , 0.39644763],\n",
              "       [0.062066  , 0.93793404],\n",
              "       [0.6256021 , 0.3743979 ],\n",
              "       [0.15172157, 0.8482784 ],\n",
              "       [0.91530484, 0.08469514],\n",
              "       [0.35345578, 0.6465442 ],\n",
              "       [0.64714247, 0.35285756],\n",
              "       [0.04041795, 0.9595821 ],\n",
              "       [0.915371  , 0.084629  ],\n",
              "       [0.04737702, 0.95262295],\n",
              "       [0.91511214, 0.08488782],\n",
              "       [0.22172725, 0.77827275],\n",
              "       [0.90426385, 0.09573616],\n",
              "       [0.19126026, 0.8087398 ],\n",
              "       [0.9154356 , 0.08456437],\n",
              "       [0.05280788, 0.94719213],\n",
              "       [0.6256021 , 0.3743979 ],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.44239268, 0.5576073 ],\n",
              "       [0.37501147, 0.6249885 ],\n",
              "       [0.91504735, 0.08495267],\n",
              "       [0.04041795, 0.9595821 ],\n",
              "       [0.535184  , 0.46481597],\n",
              "       [0.15172157, 0.8482784 ],\n",
              "       [0.91582227, 0.08417773],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.91511214, 0.08488782],\n",
              "       [0.33247992, 0.6675201 ],\n",
              "       [0.8362619 , 0.16373803],\n",
              "       [0.062066  , 0.93793404],\n",
              "       [0.89199746, 0.10800254],\n",
              "       [0.04737702, 0.95262295],\n",
              "       [0.535184  , 0.46481597],\n",
              "       [0.09378345, 0.9062165 ],\n",
              "       [0.7268955 , 0.27310455],\n",
              "       [0.10161927, 0.89838076],\n",
              "       [0.6256021 , 0.3743979 ],\n",
              "       [0.35345578, 0.6465442 ],\n",
              "       [0.7943509 , 0.20564902],\n",
              "       [0.23820917, 0.7617908 ],\n",
              "       [0.86866534, 0.13133465],\n",
              "       [0.04041795, 0.9595821 ],\n",
              "       [0.688414  , 0.31158596],\n",
              "       [0.20607713, 0.79392284],\n",
              "       [0.90426385, 0.09573616],\n",
              "       [0.17727074, 0.8227293 ],\n",
              "       [0.85902387, 0.14097606],\n",
              "       [0.05882284, 0.9411771 ],\n",
              "       [0.8231108 , 0.17688914],\n",
              "       [0.09378345, 0.9062165 ],\n",
              "       [0.9157579 , 0.08424205],\n",
              "       [0.05573904, 0.9442609 ],\n",
              "       [0.6256021 , 0.3743979 ],\n",
              "       [0.07282243, 0.9271775 ],\n",
              "       [0.48869094, 0.5113091 ],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.8485916 , 0.1514084 ],\n",
              "       [0.35345578, 0.6465442 ],\n",
              "       [0.9151769 , 0.08482305],\n",
              "       [0.37501147, 0.6249885 ],\n",
              "       [0.7622657 , 0.2377343 ],\n",
              "       [0.05002266, 0.9499774 ],\n",
              "       [0.7787244 , 0.22127563],\n",
              "       [0.20607713, 0.79392284],\n",
              "       [0.90426385, 0.09573616],\n",
              "       [0.07677455, 0.9232254 ],\n",
              "       [0.668101  , 0.33189902],\n",
              "       [0.16409688, 0.8359031 ],\n",
              "       [0.7449836 , 0.25501636],\n",
              "       [0.04257717, 0.95742285],\n",
              "       [0.5119634 , 0.48803654],\n",
              "       [0.15172157, 0.8482784 ],\n",
              "       [0.535184  , 0.46481597],\n",
              "       [0.06905862, 0.9309414 ],\n",
              "       [0.9151769 , 0.08482305],\n",
              "       [0.08114623, 0.9188537 ],\n",
              "       [0.91511214, 0.08488782],\n",
              "       [0.05002266, 0.9499774 ],\n",
              "       [0.7449836 , 0.25501636],\n",
              "       [0.27362463, 0.72637534],\n",
              "       [0.90710354, 0.09289643],\n",
              "       [0.23820917, 0.7617908 ],\n",
              "       [0.5810736 , 0.4189264 ],\n",
              "       [0.140123  , 0.859877  ],\n",
              "       [0.91582227, 0.08417773],\n",
              "       [0.11003025, 0.8899697 ],\n",
              "       [0.8231108 , 0.17688914],\n",
              "       [0.23820917, 0.7617908 ],\n",
              "       [0.5582532 , 0.44174677],\n",
              "       [0.14012301, 0.859877  ],\n",
              "       [0.91562915, 0.08437086],\n",
              "       [0.07677455, 0.9232254 ],\n",
              "       [0.9130836 , 0.08691637],\n",
              "       [0.31214792, 0.6878521 ],\n",
              "       [0.91427296, 0.08572707],\n",
              "       [0.05573904, 0.9442609 ],\n",
              "       [0.8825958 , 0.11740423],\n",
              "       [0.08653699, 0.91346294],\n",
              "       [0.9091369 , 0.09086304],\n",
              "       [0.33247992, 0.6675201 ],\n",
              "       [0.41956392, 0.5804361 ],\n",
              "       [0.35345578, 0.6465442 ],\n",
              "       [0.9155647 , 0.08443531],\n",
              "       [0.06547555, 0.9345244 ],\n",
              "       [0.7622657 , 0.2377343 ],\n",
              "       [0.10161927, 0.89838076],\n",
              "       [0.9111302 , 0.08886982],\n",
              "       [0.04486468, 0.95513535],\n",
              "       [0.7080271 , 0.29197285],\n",
              "       [0.12927605, 0.8707239 ],\n",
              "       [0.7622657 , 0.2377343 ],\n",
              "       [0.07282243, 0.9271775 ],\n",
              "       [0.7622657 , 0.2377343 ],\n",
              "       [0.33247992, 0.6675201 ],\n",
              "       [0.44239268, 0.5576073 ],\n",
              "       [0.05573904, 0.9442609 ],\n",
              "       [0.7943509 , 0.20564902],\n",
              "       [0.37501147, 0.6249885 ],\n",
              "       [0.9156935 , 0.08430645],\n",
              "       [0.25551414, 0.74448586],\n",
              "       [0.44239268, 0.5576073 ],\n",
              "       [0.19126026, 0.8087398 ],\n",
              "       [0.668101  , 0.33189902],\n",
              "       [0.23820917, 0.7617908 ],\n",
              "       [0.7268955 , 0.27310455],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.7622657 , 0.2377343 ],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.7449836 , 0.25501636],\n",
              "       [0.16409688, 0.8359031 ],\n",
              "       [0.46546736, 0.5345326 ],\n",
              "       [0.16409688, 0.8359031 ],\n",
              "       [0.6035524 , 0.39644763],\n",
              "       [0.10161927, 0.89838076],\n",
              "       [0.9153064 , 0.08469363],\n",
              "       [0.20607713, 0.79392284],\n",
              "       [0.900467  , 0.09953302],\n",
              "       [0.05280788, 0.94719213],\n",
              "       [0.688414  , 0.31158596],\n",
              "       [0.31214792, 0.6878521 ],\n",
              "       [0.668101  , 0.33189902],\n",
              "       [0.39707443, 0.60292554],\n",
              "       [0.7943509 , 0.20564902],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.7449836 , 0.25501636],\n",
              "       [0.39707443, 0.60292554],\n",
              "       [0.8825958 , 0.11740423],\n",
              "       [0.33247992, 0.6675201 ],\n",
              "       [0.8766623 , 0.1233377 ],\n",
              "       [0.07282243, 0.9271775 ],\n",
              "       [0.8091445 , 0.19085549],\n",
              "       [0.09378345, 0.9062165 ],\n",
              "       [0.86866534, 0.13133465],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.91504735, 0.08495267],\n",
              "       [0.04486468, 0.95513535],\n",
              "       [0.915371  , 0.084629  ],\n",
              "       [0.16409688, 0.8359031 ],\n",
              "       [0.90426385, 0.09573616],\n",
              "       [0.14012301, 0.859877  ],\n",
              "       [0.6256021 , 0.3743979 ],\n",
              "       [0.05882284, 0.9411771 ],\n",
              "       [0.7449836 , 0.25501636],\n",
              "       [0.09378345, 0.9062165 ],\n",
              "       [0.8362619 , 0.16373803],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.91550016, 0.08449984],\n",
              "       [0.05882284, 0.9411771 ],\n",
              "       [0.9130836 , 0.08691638],\n",
              "       [0.04737702, 0.95262295],\n",
              "       [0.8825958 , 0.11740423],\n",
              "       [0.22172725, 0.77827275],\n",
              "       [0.9157579 , 0.08424205],\n",
              "       [0.05882284, 0.9411771 ],\n",
              "       [0.8231108 , 0.17688914],\n",
              "       [0.05573904, 0.9442609 ],\n",
              "       [0.8766623 , 0.1233377 ],\n",
              "       [0.06547555, 0.9345244 ],\n",
              "       [0.9149825 , 0.08501752],\n",
              "       [0.33247992, 0.6675201 ],\n",
              "       [0.6035524 , 0.39644763],\n",
              "       [0.15172157, 0.8482784 ],\n",
              "       [0.5810736 , 0.4189264 ],\n",
              "       [0.08653699, 0.91346294],\n",
              "       [0.9130836 , 0.08691638],\n",
              "       [0.07677455, 0.9232254 ],\n",
              "       [0.8362619 , 0.16373803],\n",
              "       [0.33247992, 0.6675201 ],\n",
              "       [0.41956392, 0.5804361 ],\n",
              "       [0.05882284, 0.9411771 ],\n",
              "       [0.8825958 , 0.11740423],\n",
              "       [0.06905862, 0.9309414 ],\n",
              "       [0.7449836 , 0.25501636],\n",
              "       [0.140123  , 0.859877  ],\n",
              "       [0.6035524 , 0.39644763],\n",
              "       [0.04257717, 0.95742285],\n",
              "       [0.8485916 , 0.1514084 ],\n",
              "       [0.05280788, 0.94719213],\n",
              "       [0.8963086 , 0.10369135],\n",
              "       [0.04737702, 0.95262295],\n",
              "       [0.91530484, 0.08469514],\n",
              "       [0.17727074, 0.8227293 ],\n",
              "       [0.7622657 , 0.2377343 ],\n",
              "       [0.140123  , 0.859877  ],\n",
              "       [0.8485916 , 0.1514084 ],\n",
              "       [0.05573904, 0.9442609 ],\n",
              "       [0.8091445 , 0.19085549],\n",
              "       [0.15172157, 0.8482784 ],\n",
              "       [0.8231108 , 0.17688914],\n",
              "       [0.04041795, 0.9595821 ],\n",
              "       [0.6035524 , 0.39644763],\n",
              "       [0.10161927, 0.89838076],\n",
              "       [0.9153064 , 0.08469363],\n",
              "       [0.05280788, 0.94719213],\n",
              "       [0.6035524 , 0.39644763],\n",
              "       [0.2925145 , 0.7074855 ],\n",
              "       [0.900467  , 0.09953302],\n",
              "       [0.04257717, 0.95742285],\n",
              "       [0.915371  , 0.084629  ],\n",
              "       [0.04257717, 0.95742285],\n",
              "       [0.5119634 , 0.48803654],\n",
              "       [0.20607713, 0.79392284],\n",
              "       [0.46546736, 0.5345326 ],\n",
              "       [0.17727074, 0.8227293 ],\n",
              "       [0.688414  , 0.31158596],\n",
              "       [0.16409688, 0.8359031 ],\n",
              "       [0.91511214, 0.08488782],\n",
              "       [0.07282243, 0.9271775 ],\n",
              "       [0.8963086 , 0.10369135],\n",
              "       [0.05002266, 0.9499774 ],\n",
              "       [0.7268955 , 0.27310455],\n",
              "       [0.06547555, 0.9345244 ],\n",
              "       [0.7080271 , 0.29197285],\n",
              "       [0.04257717, 0.95742285],\n",
              "       [0.9111302 , 0.08886982],\n",
              "       [0.09378345, 0.9062165 ],\n",
              "       [0.5582532 , 0.44174677],\n",
              "       [0.06547555, 0.9345244 ],\n",
              "       [0.900467  , 0.09953302],\n",
              "       [0.07282243, 0.9271775 ],\n",
              "       [0.41956392, 0.5804361 ],\n",
              "       [0.11003025, 0.8899697 ],\n",
              "       [0.91504735, 0.08495266],\n",
              "       [0.31214792, 0.6878521 ],\n",
              "       [0.88752943, 0.11247052],\n",
              "       [0.39707443, 0.60292554],\n",
              "       [0.9153064 , 0.08469363],\n",
              "       [0.14012301, 0.859877  ],\n",
              "       [0.91550016, 0.08449984],\n",
              "       [0.07677455, 0.9232254 ],\n",
              "       [0.9157579 , 0.08424205],\n",
              "       [0.05002266, 0.9499774 ],\n",
              "       [0.9130836 , 0.08691637],\n",
              "       [0.09378345, 0.9062165 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KFh6WhdctYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rounded_prediction = model.predict_classes(scaled_test_samples, batch_size=10, verbose = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WS8wwBMc5Bh",
        "colab_type": "code",
        "outputId": "4f7a8316-2d03-43f9-a206-0ec6f3c5b5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "rounded_prediction"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUu2A0Emc6P3",
        "colab_type": "code",
        "outputId": "0c3a0c38-2312-429a-9fe0-cfe086630003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "test_samples"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 41,  86,  61,  85,  19,  97,  26,  91,  46,  95,  19,  88,  35,\n",
              "        71,  22,  88,  39,  81,  34,  90,  38,  72,  42,  82,  63,  67,\n",
              "        44,  91,  49,  97,  43,  88,  54,  99,  42,  87,  36,  80,  26,\n",
              "        97,  36,  95,  39,  70,  53,  96,  39,  78,  46,  97,  37,  98,\n",
              "        21,  81,  54,  66,  47,  73,  40,  76,  55,  91,  27,  93,  49,\n",
              "        76,  55,  70,  59,  78,  29,  77,  40,  97,  44,  79,  39,  93,\n",
              "        57,  95,  35,  82,  13,  68,  25,  90,  41,  69,  55,  99,  53,\n",
              "        79,  16,  86,  23,  95,  35, 100,  16,  95,  31,  90,  53,  81,\n",
              "        28,  70,  14,  91,  23,  82,  26,  88,  13,  72,  40,  95,  18,\n",
              "        98,  13,  88,  49, 100,  23,  75,  43,  83,  34,  71,  17,  92,\n",
              "        61,  71,  40,  80,  60,  74,  28,  84,  13,  93,  30,  65,  42,\n",
              "        99,  31,  75,  51,  75,  46,  76,  61, 100,  18,  81,  19,  72,\n",
              "        64,  85,  34,  97,  63,  74,  23,  84,  42,  88,  56,  92,  55,\n",
              "        79,  27,  67,  54, 100,  19,  97,  15,  74,  33,  76,  20,  95,\n",
              "        55,  70,  63,  66,  14, 100,  59,  79,  26,  70,  15,  68,  43,\n",
              "        92,  36,  97,  59,  85,  50,  84,  55,  67,  46,  73,  40, 100,\n",
              "        52,  75,  33,  77,  41,  93,  44,  85,  25,  94,  55,  89,  61,\n",
              "        70,  42,  67,  16,  66,  48,  96,  47,  75,  33,  88,  53,  78,\n",
              "        49,  99,  60,  79,  59,  90,  16,  87,  15,  96,  49,  71,  32,\n",
              "        73,  57,  80,  26,  83,  44,  73,  58,  80,  23,  88,  29,  69,\n",
              "        28,  94,  38,  86,  31,  68,  64,  67,  22,  91,  48,  84,  30,\n",
              "        98,  51,  81,  48,  89,  48,  68,  63,  94,  46,  66,  24,  72,\n",
              "        63,  76,  53,  73,  50,  70,  48,  70,  49,  78,  62,  78,  56,\n",
              "        84,  18,  75,  34,  95,  52,  69,  53,  65,  46,  70,  49,  65,\n",
              "        38,  68,  39,  89,  45,  85,  40,  70,  14,  98,  19,  78,  33,\n",
              "        80,  55,  93,  49,  85,  43,  70,  21,  93,  29,  97,  38,  74,\n",
              "        25,  93,  44,  94,  39,  91,  13,  68,  56,  79,  57,  86,  29,\n",
              "        88,  43,  68,  64,  93,  38,  90,  49,  80,  56,  99,  42,  95,\n",
              "        35,  97,  27,  77,  48,  80,  42,  94,  45,  79,  44, 100,  56,\n",
              "        84,  18,  95,  56,  70,  34,  99,  19,  99,  60,  75,  62,  77,\n",
              "        52,  78,  15,  89,  35,  96,  50,  91,  51,  99,  30,  85,  58,\n",
              "        91,  34,  89,  64,  83,  14,  69,  37,  65,  18,  80,  21,  88,\n",
              "        25,  96,  29,  85])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8xbxipLd1tP",
        "colab_type": "text"
      },
      "source": [
        "### Create a Confusion matrix using scikit learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMZEk6lReMkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6a5YN9GeXDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(test_labels, rounded_prediction)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vv5kIciengV",
        "colab_type": "code",
        "outputId": "4a645079-c46a-41d9-9d32-77ae344850de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "cm_labels = ['No side effects', 'Side effects observed']\n",
        "plot_confusion_matrix(cm, cm_labels, title='Confusion_Matrix')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[186  24]\n",
            " [  9 201]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAAEmCAYAAABYlZoAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nO+dcwjwRO+dcwjwRO+dcwjwRO+dcwv4f0DrARdIkJsEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_zGAZskiUBa",
        "colab_type": "text"
      },
      "source": [
        "### NOW WE ARE GONNA LEARN HOW TO SAVE OUR MODEL\n",
        "The function we use is gonna be model.save()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3-EG4_Uijso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('C:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9trj4-T2il71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}